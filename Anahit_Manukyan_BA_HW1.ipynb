{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols #for linear regression\n",
    "import seaborn as sns #for heatmap\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score #R-squared\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import r2_score, mean_squared_error#for train and test split\n",
    "from statsmodels.api import qqplot \n",
    "from scipy.stats import shapiro,ttest_ind\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan,linear_rainbow  \n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7009a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data \n",
    "\n",
    "data = pd.read_excel(r'C:\\\\Users\\\\User\\\\Downloads\\\\Student_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d08d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d024f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at the columns of the dataset\n",
    "\n",
    "print(data.columns.tolist())\n",
    "print(\" \")\n",
    "print(\"We have overall\", len(data.columns), \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1387c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of our data is:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b0639",
   "metadata": {},
   "source": [
    "When looking at the data descibe, we can some descriptive information about each variable, can think of a possible distribution. \n",
    "1. StudentID does not give us any important information from this perspective, sincce it's not made for modeling purposes. \n",
    "2. Age: looking at the description of the variable, we can see that the age of the students is between 15 to 22. The average age is about 17. Also, we can see info about the major quantiles and how the variable is spread through them. \n",
    "\n",
    "Let's skeep the other variables for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f144541",
   "metadata": {},
   "source": [
    "## Studying about and handling the missing and unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1059981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall number of missing values is\", data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall number of duplicate values is:\", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838970f1",
   "metadata": {},
   "source": [
    "As we can see, we do not have nulls and can continue working with our data without deleting the rows or making changes in the values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.columns.nunique==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b80b49",
   "metadata": {},
   "source": [
    "Also, as we can see, no variable has 1 unique value... If we had such one, we would need to derop that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab905d",
   "metadata": {},
   "source": [
    "As we can see, all of our variables are objects or integers, which makes the work easier, no need for additional work here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18624cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024ebf1",
   "metadata": {},
   "source": [
    "The above code shows the number of unique values for each variable. This helps to understand the need of modifications before dummifying the categorical variables to decrease the number of unnecessary categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76aed4",
   "metadata": {},
   "source": [
    "### Going through the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fdb9d",
   "metadata": {},
   "source": [
    "By using the following method, we can look through each variable and understand the existing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data.famsize))\n",
    "print(data.famsize.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c5e4f",
   "metadata": {},
   "source": [
    "We can run a loop and see the unique values of each variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69af172",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    print(\"The unique value in \",col,\"are:\" , data[col].unique())\n",
    "    print(col, \": \\n\", data[col].value_counts(), \"\\n ........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8bdad",
   "metadata": {},
   "source": [
    "## Understanding the Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plot and Histogram for the Final Grade \n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(\"Final Grade Distribution\",fontsize = 15, color = 'darkblue') \n",
    "sns.distplot(data.Final_Grade, hist=True, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a20889",
   "metadata": {},
   "source": [
    "Without the point 0,  the distribution of Final Grade would be similar to Normal distribution. Howevfer, a lot of students got 0 and the distribution has some gaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826dd3d",
   "metadata": {},
   "source": [
    "### Let's look at the outliers \n",
    "As the final grade has a specific range, most probably, we will not have any outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c2b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sns.boxplot(x=\"Final_Grade\",data=data, palette='cool',hue=data.paid,  showmeans=True)\n",
    "plt.legend()\n",
    "plt.title('Final grade outliers',fontsize = 15, color = 'darkblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea0d7e",
   "metadata": {},
   "source": [
    "Here we can see that the 50% quantile of final grade is about 11, here we do not have outliers. Most of the values are centered on the right side, meaning, they are higher than 10 in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57549d56",
   "metadata": {},
   "source": [
    "Let's look some other variables' distributions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"age\", \"studytime\", \"failures\", \"absences\"]:\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.scatterplot(data[i],data[\"Final_Grade\"])\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel(\"Final Grade\")\n",
    "    plt.title(f\"Relationship between {i} and the Final Grade\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab35056",
   "metadata": {},
   "source": [
    "The above cell shows scatterplots of the given variables (age, studytime, failures, absences) and final grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e511ea",
   "metadata": {},
   "source": [
    "### The following Bubble plot shows the connection between absences and final grade, where failures are mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "sns.scatterplot(data=data, x=\"absences\", y=\"Final_Grade\", size=\"failures\", \n",
    "                legend=True, sizes=(20, 1000), alpha=0.6, color = 'darkblue')\n",
    "plt.legend( prop={\"size\":20})\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c01f5",
   "metadata": {},
   "source": [
    "As we can see, as the number of failures increases, the final rade decreases. Also, the more the absences are, the lower the grade is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data.absences,data.Final_Grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c03c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data.failures,data.Final_Grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2c8ba",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5270df",
   "metadata": {},
   "source": [
    "## Separating object and numeric datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate numeric and object parts \n",
    "\n",
    "data_num = data.select_dtypes(exclude = [\"object\"])\n",
    "data_obj = data.select_dtypes(include = [\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the numeric data is:\", data_num.shape)\n",
    "print(\"The shape of the object data is:\", data_obj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f37941",
   "metadata": {},
   "source": [
    "Looking at the unique values, we can see that no varible has \"unnecessary\" category, which can be grouped with other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75cf6d",
   "metadata": {},
   "source": [
    "As we can see, in both mother's case and  father's case, healthcare job has the lowest number. So, let's group it with the \"other\" category, in order not to have so many variables after dummifying. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6585c",
   "metadata": {},
   "source": [
    "## Working with categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb82a29",
   "metadata": {},
   "source": [
    "### Let's graphically interpret the differences between the categories of object types and final grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_obj = data_obj.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for comparing Final grade with categorical variables it represented in boxplot\n",
    "#We can see difference in final grades by address,Fjob teacher level,Mjob at health level,internet, schoolsup,higher.\n",
    "for i in list_obj:\n",
    "    sns.boxplot(data=data,x=i,y=\"Final_Grade\", showmeans=True, palette='cool')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6650f",
   "metadata": {},
   "source": [
    "1. School2 \n",
    "As we can see, in terms of grades GP has higher average (median) grade, however, the grade range is higher. \n",
    "\n",
    "2. Sex\n",
    "Results show that males tend to get higher grades than females \n",
    "\n",
    "3. Address\n",
    "The students from urban areas tend to get higher scores. Their distribution is approximatelly looking like normal, and the range is smaller than that of Region students \n",
    "\n",
    "4. Famsize \n",
    "In both cases, the median is approximately equal \n",
    "\n",
    "5. Pstatus \n",
    "Apart has smaller grade range, but has an outlier \n",
    "\n",
    "\n",
    "etc. \n",
    "Let's leave the other variables, since nothing interesting can be interpreted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425af1b3",
   "metadata": {},
   "source": [
    "## Working with numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987db03",
   "metadata": {},
   "source": [
    "Let's see which variables have high correlation which do not.  \n",
    "Here, let's take 0.8 as the decision making number, meaning, if the correlation is higher or equal to 0.8 we are going to decide on keeping or deleting one of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap for examining multicollinearity\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.heatmap(data_num.corr().abs().round(2), annot =True, cmap=\"PuBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91943f2f",
   "metadata": {},
   "source": [
    "Here the variables do not have strong correlation. As we can see from the graph, none of them had >=0.8 correlation. \n",
    "Only, studentID and age have high correlation. But the variables are not related, and rationally thinking, studentID does not have any impact on the final grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce4302",
   "metadata": {},
   "source": [
    "Walc and Dalc have the seond highest values. Logically, they can have some correlation, in a sense that a person likes drinking alcohol or not. However, the results are less than the target value, so let's leave the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17866c4",
   "metadata": {},
   "source": [
    "Since we do not need ID variable for the modeling, let's remove that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d68ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"StudentID\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24316578",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data_num.drop(\"StudentID\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc83885",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "sns.pairplot(data_num)\n",
    "\n",
    "\n",
    "# Visual representation of pairwise relationships\n",
    "# The plot is made to see the highest correlated variables with the Final Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042ffa9",
   "metadata": {},
   "source": [
    "The graph shows the linear relationship between the Final Grade and all the numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f1a14",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc40f7",
   "metadata": {},
   "source": [
    "Since about half of our variables are categorical (in object type), we can convert their meaning into numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8c462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's dymmify the variables\n",
    "\n",
    "data_dummies = pd.get_dummies(data_obj, drop_first = True)\n",
    "data_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84905200",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = data_num.join(data_dummies, how = 'left')\n",
    "data_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_model.Final_Grade\n",
    "X = data_model.drop('Final_Grade', axis = 1) # we drop Final Grade in order not to have the target in the X part  \n",
    "X = sm.add_constant(X) # add constant as an intercept "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1a42c",
   "metadata": {},
   "source": [
    "Let's split our data into two parts, where 75% of our data will be used in the train part and 25% in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59598e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split data \n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.25, random_state=17)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8addfa1f",
   "metadata": {},
   "source": [
    "As we can see, the number of columns has decreases by 1, which was for Final Grade, target variable \n",
    "And we took 296 values for the train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3979152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's create the linear model \n",
    "\n",
    "model_linear = sm.OLS(train_y, train_x)\n",
    "results = model_linear.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb345d94",
   "metadata": {},
   "source": [
    "Let's look at each point one by one: \n",
    "\n",
    "1. R-squared is low, meaning, our independent variable is not explaining much in the variation of your dependent variable - regardless of the variable significance\n",
    "2. Adj. R-squared is even lower, which indicates that the additional input variables are not adding value to the model.\n",
    "3. F-stat is close to 0, which is good\n",
    "4. We will use AIC and BIC later, when we have different models and we will need to take one with higher AIC and BIC values (if R-squared is not informative) \n",
    "\n",
    "Now, let's consider t and p-value, from which we can understan that const is not significant  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fdb8e",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aba209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prediction on the test set\n",
    "y_pred=results.predict(test_x)\n",
    "print('R^2_test:', r2_score(test_y, y_pred))\n",
    "print('RMSE:', mean_squared_error(test_y, y_pred)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873e272",
   "metadata": {},
   "source": [
    "The difference between train and test values is on average 4.31 point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96d7b9",
   "metadata": {},
   "source": [
    "F-stat is too small, near to 0, which means that our model is statistically significant at 5%(even 1%) significance level, so there is at least one estimated coefficient (besides intercept) that is not null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc33d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R-squared shows that 33.5% of variance in Final Grades is explained by the variables included in the model.\n",
    "\n",
    "#Adj. R-squared 24% is not the same as (or close to) R-squared, which means that not all of the included variables are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ea89e",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cba1f2",
   "metadata": {},
   "source": [
    "As we can see, the model is not strong enough. So, let's remove some of the unnecessary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb20d9c",
   "metadata": {},
   "source": [
    "Let's understand which variable is important and which is not, to remove them correctly and go to the estimation part again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f1da1",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eae459",
   "metadata": {},
   "source": [
    "First model is complete, let's continue with feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf6d55",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cae6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformm to the binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6916ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate some unsignificant features according to p-value\n",
    "p_values = results.pvalues.round(3).reset_index().rename(\n",
    "    columns={\n",
    "        'index':'features',\n",
    "        0: 'p_value'\n",
    "    }\n",
    ").sort_values(\"p_value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate features that have more than 0.5 p_value\n",
    "bad_features_df = p_values[p_values.p_value > 0.5]\n",
    "bad_features = bad_features_df.features.tolist()\n",
    "print(bad_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2783151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate features that have more than 0.5 p_value\n",
    "bad_features_df = p_values[p_values.p_value > 0.5]\n",
    "bad_features = bad_features_df.features.tolist()\n",
    "print(bad_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbe427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall we have\", len(bad_features), \"bad features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we know what features are bud let's run multiple regressions without them (eliminateing one by one)\n",
    "for feature in bad_features:\n",
    "    # temporary variables to save the new dataset\n",
    "    temporary_train_x = train_x.drop([feature], axis = 1)\n",
    "    temporary_test_x = test_x.drop([feature], axis = 1)\n",
    "    # temporary models to see how it changes if bad features are eliminated\n",
    "    temporary_model = sm.OLS(train_y, temporary_train_x)\n",
    "    temporary_results = temporary_model.fit()\n",
    "    print(f\"This is a evaluation of the model without {feature.upper()} feature\")\n",
    "    print(\"Train R^2:\",temporary_results.rsquared.round(3))\n",
    "    print(\"Train R^2 Adjusted:\",temporary_results.rsquared_adj.round(3))\n",
    "    # check prediction on the test set\n",
    "    temporary_y_pred=temporary_results.predict(temporary_test_x)\n",
    "    print('R^2_test:', r2_score(test_y, temporary_y_pred).round(3))\n",
    "    print('RMSE test:', (mean_squared_error(test_y, temporary_y_pred)**0.5).round(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9d482",
   "metadata": {},
   "source": [
    "As we can see, the insignificant features have so little impact on our model so eliminating them will better the model(though slightly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634e228",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3970891",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data_model.Dalc, data_model.Walc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11548e07",
   "metadata": {},
   "source": [
    "Dalc and Walc have some correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba827dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dalc and Walc have similar meaning, so we can work on them \n",
    "#New variable will represent  weekly alcohol consumption\n",
    "data_model_new = data_model\n",
    "\n",
    "data_model_new[\"alc\"]=5/7*data_model_new[\"Dalc\"]+2/7*data_model_new[\"Walc\"]\n",
    "\n",
    "# we take 5 for weekdays and 2 for weekends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_model_new.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7055a1",
   "metadata": {},
   "source": [
    "As we can see, a new column is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b31e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thus we will merge freetime, traveltime and free time after school by taking mean of the two columns\n",
    "data_model_new[\"Freetime\"]=data_model_new[['freetime', 'goout', 'traveltime']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e664f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_model_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e7967",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18fb30",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split data\n",
    "Y = data_model_new.Final_Grade\n",
    "X = data_model_new.drop('Final_Grade', axis = 1)\n",
    "X = sm.add_constant(X)\n",
    "train_engineered_x, test_engineered_x, train_engineered_y, test_engineered_y = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "train_engineered_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ade003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adcfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observing the results\n",
    "results2=sm.OLS(Y_train, X_train).fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prediction on the test set (initial)\n",
    "y_pred=results.predict(test_x)\n",
    "print('R^2_test:', r2_score(test_y, y_pred))\n",
    "print('RMSE:', mean_squared_error(test_y, y_pred)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4262d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prediction on the test set\n",
    "y_engineered_pred=results2.predict(test_engineered_x)\n",
    "print('R^2_test:', r2_score(test_engineered_y, y_engineered_pred))\n",
    "print('RMSE:', mean_squared_error(test_engineered_y, y_engineered_pred)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef06ef",
   "metadata": {},
   "source": [
    "As we can see, here we have big positive change in terms R-squared. \n",
    "And RMSE is decreased, which is also a good sign. \n",
    "\n",
    "Also, AIC and BIC have better results, since, \n",
    "<br>   initial: AIC=1718, BIC=1866\n",
    "<br>   new model: AIC=1938, BIC=2090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbd90e",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77af50",
   "metadata": {},
   "source": [
    "#### Interpretation of the results of Linear Regression\n",
    "F-statistics (2.27e-07) : As Prob (F-statistic) shows our model is statistically significant at 5%(even 1%) significance level, so there is at least one estimated coefficient (besides intercept) that is not null.\n",
    "\n",
    "R-squared has low value and which shows that only 27.5% of variance in final grade is explained by the variables included in the model.\n",
    "\n",
    "Adj. R-squared: As summary shows it's 18%. It is not quite close to R-squared, which means that there are not important variables included in the model, with which we are going to work in the next part. \n",
    "\n",
    "Statistically significant variables: \n",
    "  - **failures** has a **p-value** equal to zero which means that it is statsitically significant, and as the coefficient is positive it's effect on price is positive.\n",
    "  \n",
    "  Other variables, such as **Studytime, sexM, famsup_Yes, and romantic_Yes** are considered to be important variables. \n",
    "  - Other variables are not statistically significant at 95% conidence interval, but some are significant at 90%. \n",
    "  \n",
    "Final Model\n",
    "\n",
    "Please note that if the variable is in the model and you have no possibility or you do not want to remove it it should be in the final model formulation even if it is not statistically significant\n",
    "\n",
    "**Final Grade = 12.4613 - 0.3100 age + 0.3915 Medu - 0.0815 Fedu + ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1e442",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85affbb",
   "metadata": {},
   "source": [
    "### Let's eliminate some unsignificant features according to p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d09fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = results2.pvalues.round(3).reset_index().rename(\n",
    "    columns={\n",
    "        'index':'features',\n",
    "        0: 'p_value'\n",
    "    }\n",
    ").sort_values(\"p_value\")\n",
    "p_values.fillna(1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de29e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate features that have more than 0.5 p_value\n",
    "bad_features_after_engineering_df = p_values[p_values.p_value > 0.5]\n",
    "bad_features_after_engineering = bad_features_after_engineering_df.features.tolist()\n",
    "print(bad_features_after_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we know what features are bud let's run multiple regressions without them (eliminateing one by one)\n",
    "feature_list = list()\n",
    "for feature in bad_features:\n",
    "    feature_list.append(feature)\n",
    "    # temporary variables to save the new dataset\n",
    "    temporary_train_x = train_engineered_x.drop(feature_list, axis = 1)\n",
    "    temporary_test_x = test_engineered_x.drop(feature_list, axis = 1)\n",
    "    # temporary models to see how it changes if bad features are eliminated\n",
    "    temporary_model = sm.OLS(train_engineered_y, temporary_train_x)\n",
    "    temporary_results = temporary_model.fit()\n",
    "    print(f\"This is a evaluation of the model without {feature.upper()} feature\")\n",
    "    print(\"Train R^2:\",temporary_results.rsquared.round(3))\n",
    "    print(\"Train R^2 Adjusted:\",temporary_results.rsquared_adj.round(3))\n",
    "    # check prediction on the test set\n",
    "    temporary_y_pred=temporary_results.predict(temporary_test_x)\n",
    "    print('R^2_test:', r2_score(test_engineered_y, temporary_y_pred).round(3))\n",
    "    print('RMSE test:', (mean_squared_error(test_engineered_y, temporary_y_pred)**0.5).round(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e90405",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_model = temporary_results.summary()\n",
    "Final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify do not fit intercept because we already have the constant feature\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(train_engineered_x, train_engineered_y)\n",
    "print('R^2_train:', reg.score(test_engineered_x, test_engineered_y))\n",
    "print('R^2_train:', reg.score(test_engineered_x, test_engineered_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c938029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the package do not have the same summary, and the code below only provides the features and respective coefficients\n",
    "coef = pd.DataFrame(dict(zip(train_engineered_x.columns, reg.coef_)), index=[0]).T\n",
    "coef.columns=['coef']\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18b0d5",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c942d",
   "metadata": {},
   "source": [
    "## Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residuals of the model\n",
    "residuals1=results2.resid\n",
    "predicted_values1=results2.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc580781",
   "metadata": {},
   "source": [
    "### 1. Homoscedasticity assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8690ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted values vs residuals\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(predicted_values1,residuals1)\n",
    "plt.axhline(y=0, c=\"red\")\n",
    "plt.title(\"Residuals vs Predicted values for backward eliminated model\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breusch-Pagan test for homoscedasticity\n",
    "bnames = ['Lagrange multiplier statistic (for homoscedasticity)', 'p-value','f-value', 'f p-value']\n",
    "breush = het_breuschpagan(residuals1, results2.model.exog)\n",
    "print(list(zip(bnames, breush)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb545a7a",
   "metadata": {},
   "source": [
    "#### Homoscedasticity assumptions -- fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237f64f",
   "metadata": {},
   "source": [
    "### 2. Normality assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normality\n",
    "sns.distplot(residuals1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk normality test. \n",
    "#Normality assumption isn't held.\n",
    "snames=['The test statistic for Normality', 'p-value']\n",
    "shapiro_test=shapiro(residuals1)\n",
    "print(list(zip(snames, shapiro_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a508ea6",
   "metadata": {},
   "source": [
    "#### Normality assumption -- fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b402e7",
   "metadata": {},
   "source": [
    "### 3. Linearity assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911807cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QQ plot\n",
    "qqplot(residuals1,fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e2236",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d59ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linearity \n",
    "rnames=[\"fstat\", \"p-value\"]\n",
    "rainbow=linear_rainbow(results2)\n",
    "print(list(zip(rnames, rainbow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbc689",
   "metadata": {},
   "source": [
    "#### Linearity assumption is held"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb55e2",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa974a",
   "metadata": {},
   "source": [
    "## Let's check multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904b39c",
   "metadata": {},
   "source": [
    "#### VIF: measure of the amount of multicollinearity in a set of multiple regression variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2defab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list=X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d66d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multicollinearity\n",
    "#calculating vif using variance_inflation_factor() function from statsmodel\n",
    "vif = [variance_inflation_factor(X[x_list].values, i) for i in range(0,len(x_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vif values with variable names\n",
    "#No multicollinearity\n",
    "for i in range(0,len(x_list)):\n",
    "    print(x_list[i],\":\",vif[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a9824",
   "metadata": {},
   "source": [
    "Although, the vif of const is too high, let's ignore that, since, it is mannually added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838bba4",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
